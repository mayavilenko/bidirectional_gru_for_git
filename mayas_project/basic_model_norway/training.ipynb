{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-18 13:09:04.678470: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "#from transformers import AdamW\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "import itertools\n",
    "\n",
    "from GRU_model import *\n",
    "from config import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeds for Comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(2)\n",
    "random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08071971994256645"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_dict = pd.read_pickle(train_dataset_dict_path)\n",
    "test_dataset_dict = pd.read_pickle(test_dataset_dict_path)\n",
    "category_id_to_name_dict = pd.read_pickle(category_id_to_category_name_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(category_train_df, category_test_df):\n",
    "    x_train = category_train_df.iloc[:,:-1].to_numpy()\n",
    "    y_train = category_train_df.iloc[:,-1].to_numpy()\n",
    "    x_test = category_test_df.iloc[:,:-1].to_numpy()\n",
    "    y_test= category_test_df.iloc[:,-1].to_numpy()\n",
    "\n",
    "    x_train= torch.from_numpy(x_train).to(torch.float32)\n",
    "    y_train = torch.from_numpy(y_train).to(torch.float32)\n",
    "    x_test = torch.from_numpy(x_test).to(torch.float32)\n",
    "    y_test = torch.from_numpy(y_test).to(torch.float32)\n",
    "\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    test_dataset = TensorDataset(x_test, y_test)\n",
    "    train_dataloader =  DataLoader(train_dataset, batch_size=BatchSize, shuffle=False)\n",
    "    test_dataloader =  DataLoader(test_dataset, batch_size=BatchSize, shuffle=False)\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, train_dataloader, optimizer):\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    predictions_list = []\n",
    "    for inputs, labels in train_dataloader:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        #Changing input shape - last batch size can change so we define it as input.shape[0]\n",
    "        inputs = inputs.view(inputs.shape[0], SequenceLength, Features) \n",
    "        #model prediction\n",
    "        pred = model(inputs)\n",
    "        #append batch predictions to predictions list\n",
    "        predictions_list.append(pred.view(1,-1))\n",
    "        # calculate loss\n",
    "        loss = Criterion(pred, labels.view(-1,1))\n",
    "        # calculate the gradient\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        #Add to loss of batch to epoch train loss\n",
    "        running_loss+=loss.item()\n",
    "    # Calculte the epoch train loss\n",
    "    epoch_train_loss = running_loss/len(train_dataloader.dataset)\n",
    "    return epoch_train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_loop(model, test_dataloader):\n",
    "     # Evaluation\n",
    "    # Initiate test loss, accuracy and f1 score to zero\n",
    "    test_loss = 0\n",
    "    # Change model to eval mode\n",
    "    model.eval()\n",
    "    # we dont need to update weights, so we define no_grad() to save memory\n",
    "\n",
    "    predictions_list = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs = inputs.view(inputs.shape[0], SequenceLength, Features)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            out = model(inputs)\n",
    "            predictions_list.append(out.view(1,-1))\n",
    "            test_batch_loss = Criterion(out, labels.view(-1,1))\n",
    "            test_loss += test_batch_loss.item()\n",
    "    # Calculate epoch loss\n",
    "    epoch_predictions = torch.cat(predictions_list, dim=1)\n",
    "    epoch_test_loss = test_loss/len(test_dataloader.dataset)\n",
    "        \n",
    "    return epoch_test_loss, epoch_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint, is_best, checkpoint_path, best_model_path):\n",
    "    \"\"\"\n",
    "    checkpoint: checkpoint we want to save\n",
    "    is_best: is this the best checkpoint; min validation loss\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    best_model_path: path to save best model\n",
    "    \"\"\"\n",
    "    # save checkpoint data to the path given, checkpoint_path\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    # if it is a best model, min validation loss\n",
    "    if is_best:\n",
    "        # copy that checkpoint file to best path given, best_model_path\n",
    "        shutil.copyfile(checkpoint_path, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    \"\"\"\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into       \n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    # initialize optimizer from checkpoint to optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
    "    valid_loss_min = checkpoint['valid_loss_min']\n",
    "    # return model, optimizer, epoch value, min validation loss \n",
    "    return model, optimizer, checkpoint['epoch'], valid_loss_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_model_weights(model):\n",
    "    param_dict ={}\n",
    "    for name, param in model.named_parameters():\n",
    "        param_dict[name] = param\n",
    "\n",
    "    param_dict['gru.bias_hh_l0'] = param_dict['gru.bias_hh_l0'].view(-1,1)\n",
    "    param_dict['gru.bias_ih_l0'] = param_dict['gru.bias_ih_l0'].view(-1,1)\n",
    "    unified_weights = torch.hstack((\n",
    "            param_dict['gru.weight_ih_l0'],\n",
    "            param_dict['gru.weight_hh_l0'],\n",
    "            param_dict['gru.bias_ih_l0'],\n",
    "            param_dict['gru.bias_hh_l0']))\n",
    "\n",
    "    return unified_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_and_evaluation(model, train_dataloader, test_dataloader, optim, category, checkpoint_path, best_checkpoint_path):\n",
    "   #results list\n",
    "   train_loss_list = []\n",
    "   test_loss_list = []\n",
    "\n",
    "   ##Create writer for using tesndorboard\n",
    "   #writer = SummaryWriter(log_dir=f'{TbDirectory}_{category}')\n",
    "\n",
    "   min_test_loss = np.inf\n",
    "\n",
    "   for epoch in range(Epochs):\n",
    "      #initiate train epoch loss\n",
    "      epoch_train_loss = training_loop(model, train_dataloader, optim)\n",
    "      epoch_test_loss, epoch_test_predictions = evaluation_loop(model, test_dataloader)\n",
    "\n",
    "      checkpoint = {\n",
    "         'epoch': epoch + 1,\n",
    "         'valid_loss_min': epoch_test_loss,\n",
    "         'state_dict': model.state_dict(),\n",
    "         'optimizer': optim.state_dict(),\n",
    "        }\n",
    "      \n",
    "      # save checkpoint\n",
    "      save_checkpoint(checkpoint, False, checkpoint_path, best_checkpoint_path)\n",
    "\n",
    "      if epoch_test_loss <= min_test_loss:\n",
    "         save_checkpoint(checkpoint, True, checkpoint_path, best_checkpoint_path)\n",
    "         min_test_loss = epoch_test_loss\n",
    "\n",
    "      train_loss_list.append(epoch_train_loss)\n",
    "      test_loss_list.append(epoch_test_loss)\n",
    "\n",
    "      ## Display those measures on tensorboard\n",
    "      #writer.add_scalar(tag='loss/train', scalar_value=epoch_train_loss, global_step=epoch)\n",
    "      #writer.add_scalar(tag='loss/test', scalar_value=epoch_test_loss, global_step=epoch)\n",
    "    \n",
    "   results = {'train_loss': train_loss_list, 'test_loss': test_loss_list} \n",
    "   return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(category_id_to_name_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accommodation services',\n",
       " 'Actual rentals for housing',\n",
       " 'Alcoholic beverages',\n",
       " 'Alcoholic beverages and tobacco',\n",
       " 'All-items',\n",
       " 'Audio-visual, photographic and information processing equipment',\n",
       " 'Clothing',\n",
       " 'Clothing and footwear',\n",
       " 'Communications',\n",
       " 'Education',\n",
       " 'Electricity, gas and other fuels',\n",
       " 'Financial services n.e.c.',\n",
       " 'Food',\n",
       " 'Food and non-alcoholic beverages',\n",
       " 'Footwear',\n",
       " 'Furnishings, household equipment and routine maintenance',\n",
       " 'Furniture and furnishings, carpets and other floor coverings',\n",
       " 'Glassware, tableware and household utensils',\n",
       " 'Goods and services for routine household maintenance',\n",
       " 'Health',\n",
       " 'Household appliances',\n",
       " 'Household textiles',\n",
       " 'Housing, water, electricity, gas and other fuels',\n",
       " 'Imputed rentals for housing',\n",
       " 'Insurance',\n",
       " 'Maintenance and repair of the dwelling',\n",
       " 'Medical products, appliances and equipment',\n",
       " 'Miscellaneous goods and services',\n",
       " 'Newspapers, books and stationery',\n",
       " 'Non-alcoholic beverages',\n",
       " 'Operation of personal transport equipment',\n",
       " 'Other major durables for recreation and culture',\n",
       " 'Other recreational items and equipment, gardens and pets',\n",
       " 'Other services n.e.c.',\n",
       " 'Out-patient services',\n",
       " 'Package holidays',\n",
       " 'Personal care',\n",
       " 'Personal effects n.e.c.',\n",
       " 'Postal services',\n",
       " 'Purchase of vehicles',\n",
       " 'Recreation and culture',\n",
       " 'Recreational and cultural services',\n",
       " 'Restaurant services',\n",
       " 'Restaurants and hotels',\n",
       " 'Social protection',\n",
       " 'Telephone equipment',\n",
       " 'Telephone services',\n",
       " 'Tobacco',\n",
       " 'Tools and equipment for house and garden',\n",
       " 'Transport',\n",
       " 'Transport services',\n",
       " 'Water supply and miscellaneous services relating to the dwelling']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipline(train_dataset_dict, test_dataset_dict):\n",
    "    results = {}\n",
    "    num_categories = 0\n",
    "    for category_id in list(category_id_to_name_dict.keys()):\n",
    "        num_categories +=1\n",
    "        print(f'num categories: {num_categories}')\n",
    "        category = category_id_to_name_dict[category_id]\n",
    "        print(f'category id|name: {category_id}|{category}')\n",
    "\n",
    "        train_dataloader, test_dataloader = create_dataloader(train_dataset_dict[category], test_dataset_dict[category])\n",
    "\n",
    "        model = GRUModel(input_dim = Features, hidden_dim = HiddenSize, layer_dim = LayersDim, output_dim = OutputDim, dropout_prob = DropoutProb)\n",
    "        model.to(device)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=Lr)\n",
    "\n",
    "        parameters_file_name = category+'.pt'\n",
    "        \n",
    "        results[category] = training_and_evaluation(\n",
    "                                model=model,\n",
    "                                optim=optimizer,\n",
    "                                train_dataloader=train_dataloader,\n",
    "                                test_dataloader=test_dataloader,\n",
    "                                category=category,\n",
    "                                checkpoint_path=CheckpointPath+parameters_file_name,\n",
    "                                best_checkpoint_path=BestcheckpointPath+parameters_file_name,\n",
    "                            )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num categories: 1\n",
      "category id|name: 45|Accommodation services\n",
      "num categories: 2\n",
      "category id|name: 19|Actual rentals for housing\n",
      "num categories: 3\n",
      "category id|name: 15|Alcoholic beverages\n",
      "num categories: 4\n",
      "category id|name: 2|Alcoholic beverages and tobacco\n",
      "num categories: 5\n",
      "category id|name: 0|All-items\n",
      "num categories: 6\n",
      "category id|name: 38|Audio-visual, photographic and information processing equipment\n",
      "num categories: 7\n",
      "category id|name: 17|Clothing\n",
      "num categories: 8\n",
      "category id|name: 3|Clothing and footwear\n",
      "num categories: 9\n",
      "category id|name: 8|Communications\n",
      "num categories: 10\n",
      "category id|name: 10|Education\n",
      "num categories: 11\n",
      "category id|name: 23|Electricity, gas and other fuels\n",
      "num categories: 12\n",
      "category id|name: 50|Financial services n.e.c.\n",
      "num categories: 13\n",
      "category id|name: 13|Food\n",
      "num categories: 14\n",
      "category id|name: 1|Food and non-alcoholic beverages\n",
      "num categories: 15\n",
      "category id|name: 18|Footwear\n",
      "num categories: 16\n",
      "category id|name: 5|Furnishings, household equipment and routine maintenance\n",
      "num categories: 17\n",
      "category id|name: 24|Furniture and furnishings, carpets and other floor coverings\n",
      "num categories: 18\n",
      "category id|name: 27|Glassware, tableware and household utensils\n",
      "num categories: 19\n",
      "category id|name: 29|Goods and services for routine household maintenance\n",
      "num categories: 20\n",
      "category id|name: 6|Health\n",
      "num categories: 21\n",
      "category id|name: 26|Household appliances\n",
      "num categories: 22\n",
      "category id|name: 25|Household textiles\n",
      "num categories: 23\n",
      "category id|name: 4|Housing, water, electricity, gas and other fuels\n",
      "num categories: 24\n",
      "category id|name: 20|Imputed rentals for housing\n",
      "num categories: 25\n",
      "category id|name: 49|Insurance\n",
      "num categories: 26\n",
      "category id|name: 21|Maintenance and repair of the dwelling\n",
      "num categories: 27\n",
      "category id|name: 30|Medical products, appliances and equipment\n",
      "num categories: 28\n",
      "category id|name: 12|Miscellaneous goods and services\n",
      "num categories: 29\n",
      "category id|name: 42|Newspapers, books and stationery\n",
      "num categories: 30\n",
      "category id|name: 14|Non-alcoholic beverages\n",
      "num categories: 31\n",
      "category id|name: 33|Operation of personal transport equipment\n",
      "num categories: 32\n",
      "category id|name: 39|Other major durables for recreation and culture\n",
      "num categories: 33\n",
      "category id|name: 40|Other recreational items and equipment, gardens and pets\n",
      "num categories: 34\n",
      "category id|name: 51|Other services n.e.c.\n",
      "num categories: 35\n",
      "category id|name: 31|Out-patient services\n",
      "num categories: 36\n",
      "category id|name: 43|Package holidays\n",
      "num categories: 37\n",
      "category id|name: 46|Personal care\n",
      "num categories: 38\n",
      "category id|name: 47|Personal effects n.e.c.\n",
      "num categories: 39\n",
      "category id|name: 35|Postal services\n",
      "num categories: 40\n",
      "category id|name: 32|Purchase of vehicles\n",
      "num categories: 41\n",
      "category id|name: 9|Recreation and culture\n",
      "num categories: 42\n",
      "category id|name: 41|Recreational and cultural services\n",
      "num categories: 43\n",
      "category id|name: 44|Restaurant services\n",
      "num categories: 44\n",
      "category id|name: 11|Restaurants and hotels\n",
      "num categories: 45\n",
      "category id|name: 48|Social protection\n",
      "num categories: 46\n",
      "category id|name: 36|Telephone equipment\n",
      "num categories: 47\n",
      "category id|name: 37|Telephone services\n",
      "num categories: 48\n",
      "category id|name: 16|Tobacco\n",
      "num categories: 49\n",
      "category id|name: 28|Tools and equipment for house and garden\n",
      "num categories: 50\n",
      "category id|name: 7|Transport\n",
      "num categories: 51\n",
      "category id|name: 34|Transport services\n",
      "num categories: 52\n",
      "category id|name: 22|Water supply and miscellaneous services relating to the dwelling\n"
     ]
    }
   ],
   "source": [
    "results = pipline(train_dataset_dict, test_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/model_results.pickle', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Best Model Per Category Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_of_best_model_per_category(categories_list, dir_path):\n",
    "    basic_model = GRUModel(input_dim = Features, hidden_dim = HiddenSize, layer_dim = LayersDim, output_dim = OutputDim, dropout_prob = DropoutProb)\n",
    "    basic_optimizer = torch.optim.AdamW(basic_model.parameters(), lr=Lr)\n",
    "    basic_model.to(device)\n",
    "\n",
    "    best_models_dict = {}\n",
    "\n",
    "    for category in categories_list:\n",
    "        ckp_path = dir_path+category+'.pt'\n",
    "        model, optimizer, checkpoint, valid_loss_min = load_checkpoint(ckp_path, basic_model, basic_optimizer)\n",
    "        best_models_dict[category] = model\n",
    "        \n",
    "    return best_models_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_list = categories\n",
    "dir_path = \"checkpoints/best_checkpoints/\"\n",
    "\n",
    "best_models_dict = create_dict_of_best_model_per_category(categories_list, dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accommodation services': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Actual rentals for housing': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Alcoholic beverages': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Alcoholic beverages and tobacco': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'All-items': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Audio-visual, photographic and information processing equipment': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Clothing': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Clothing and footwear': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Communications': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Education': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Electricity, gas and other fuels': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Financial services n.e.c.': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Food': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Food and non-alcoholic beverages': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Footwear': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Furnishings, household equipment and routine maintenance': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Furniture and furnishings, carpets and other floor coverings': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Glassware, tableware and household utensils': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Goods and services for routine household maintenance': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Health': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Household appliances': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Household textiles': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Housing, water, electricity, gas and other fuels': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Imputed rentals for housing': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Insurance': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Maintenance and repair of the dwelling': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Medical products, appliances and equipment': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Miscellaneous goods and services': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Newspapers, books and stationery': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Non-alcoholic beverages': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Operation of personal transport equipment': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Other major durables for recreation and culture': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Other recreational items and equipment, gardens and pets': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Other services n.e.c.': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Out-patient services': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Package holidays': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Personal care': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Personal effects n.e.c.': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Postal services': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Purchase of vehicles': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Recreation and culture': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Recreational and cultural services': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Restaurant services': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Restaurants and hotels': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Social protection': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Telephone equipment': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Telephone services': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Tobacco': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Tools and equipment for house and garden': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Transport': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Transport services': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " ),\n",
       " 'Water supply and miscellaneous services relating to the dwelling': GRUModel(\n",
       "   (gru): GRU(1, 64, batch_first=True)\n",
       "   (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       " )}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Best Predictions for Each Category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_predictions_for_each_category(best_models_dict):\n",
    "    best_predictions_dict = {}\n",
    "\n",
    "    for category in list(best_models_dict.keys()):\n",
    "        model = best_models_dict[category]\n",
    "        train_dataloader, test_dataloader = create_dataloader(train_dataset_dict[category], test_dataset_dict[category])\n",
    "        epoch_test_loss, epoch_predictions = evaluation_loop(model, test_dataloader)\n",
    "        best_predictions_dict[category] = epoch_predictions\n",
    "\n",
    "    return best_predictions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_predictions_dict = get_best_predictions_for_each_category(best_models_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/predictions_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(best_predictions_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Best Model Weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_per_category(category_list, dir_path):\n",
    "    weights = {}\n",
    "    for category in category_list:\n",
    "        model = GRUModel(input_dim = Features, hidden_dim = HiddenSize, layer_dim = LayersDim, output_dim = OutputDim, dropout_prob = DropoutProb)\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=Lr)\n",
    "        \n",
    "        best_checkpoint_path = dir_path + category + '.pt'\n",
    "\n",
    "        category_model, optimizer, checkpoint, valid_loss_min = load_checkpoint(best_checkpoint_path, model, optimizer)\n",
    "        category_model_weights = unify_model_weights(category_model)\n",
    "\n",
    "        weights[category] = category_model_weights\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_per_category(category_id_list, dir_path):\n",
    "    basic_model = GRUModel(input_dim = Features, hidden_dim = HiddenSize, layer_dim = LayersDim, output_dim = OutputDim, dropout_prob = DropoutProb)\n",
    "    basic_optimizer = torch.optim.AdamW(basic_model.parameters(), lr=Lr)\n",
    "    basic_model.to(device)\n",
    "\n",
    "    best_models_weights_dict = {}\n",
    "\n",
    "    for category_id in category_id_list:\n",
    "        category_name = category_id_to_name_dict[category_id]\n",
    "        ckp_path = dir_path+category_name+'.pt'\n",
    "        model, optimizer, checkpoint, valid_loss_min = load_checkpoint(ckp_path, basic_model, basic_optimizer)\n",
    "        category_model_weights = unify_model_weights(model)\n",
    "        best_models_weights_dict[category_id] = category_model_weights\n",
    "        \n",
    "    return best_models_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/Users/mvilenko/Library/CloudStorage/OneDrive-PayPal/CPI_HRNN - version 2.0/mayas_project/basic_model_norway/checkpoints/best_checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id_list = []\n",
    " \n",
    "# list out keys and values separately\n",
    "key_list = list(category_id_to_name_dict.keys())\n",
    "val_list = list(category_id_to_name_dict.values())\n",
    "\n",
    "for cat_name in categories:\n",
    "    position = val_list.index(cat_name)\n",
    "    category_id_list.append(key_list[position])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dict = get_weights_per_category(category_id_list, dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/mvilenko/Library/CloudStorage/OneDrive-PayPal/CPI_HRNN - version 2.0/pickle files/norway_sgru_model_weights.pickle', 'wb') as handle:\n",
    "    pickle.dump(weights_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{45: tensor([[-1.0324e-01, -4.2914e-01, -8.3811e-02,  ..., -1.0002e-03,\n",
       "          -9.6098e-01, -8.6348e-01],\n",
       "         [-1.5876e+00,  6.5696e-01, -3.9444e-01,  ..., -2.7079e-01,\n",
       "          -1.7413e+00, -1.6642e+00],\n",
       "         [ 7.8310e-01,  6.0179e-01,  2.9628e-01,  ...,  2.9923e-01,\n",
       "          -6.0128e-01, -5.7441e-01],\n",
       "         ...,\n",
       "         [-5.4172e-01, -7.4301e-01, -3.8378e-01,  ...,  7.5334e-01,\n",
       "           7.2861e-03, -1.9579e-01],\n",
       "         [ 8.2305e-01, -4.0005e-02,  1.9541e-01,  ...,  3.2599e-01,\n",
       "          -1.4596e+00, -6.5245e-01],\n",
       "         [ 3.4290e-02,  6.4210e-01, -1.5608e-01,  ...,  3.6539e-02,\n",
       "          -1.7264e+00, -1.0241e+00]], grad_fn=<CatBackward0>),\n",
       " 19: tensor([[ 0.0813, -0.7061, -0.9026,  ..., -0.0651, -0.1227, -0.0775],\n",
       "         [-0.3400, -0.0922,  0.1280,  ..., -0.0664, -0.1752, -0.1828],\n",
       "         [-0.1504,  0.0319, -0.0244,  ..., -0.0558, -0.0686, -0.0922],\n",
       "         ...,\n",
       "         [-0.0113, -0.9250, -0.1227,  ...,  0.2479,  0.0667,  0.1201],\n",
       "         [-0.0023,  0.3247,  0.0504,  ...,  0.0878, -0.0308,  0.0142],\n",
       "         [-0.0374,  0.2187,  0.1443,  ..., -0.2318, -0.0486, -0.1186]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 15: tensor([[ 0.0974, -0.1192,  0.1426,  ..., -0.2303, -0.3932, -0.4250],\n",
       "         [ 0.0327, -0.1467,  0.0815,  ..., -0.1366, -0.3398, -0.2816],\n",
       "         [-0.3480, -0.5163,  0.6958,  ..., -0.5795,  0.1111,  0.1028],\n",
       "         ...,\n",
       "         [-0.2649, -0.1105,  0.0898,  ...,  0.0556,  0.0056,  0.0380],\n",
       "         [-0.4478, -0.1283,  0.0272,  ...,  0.0172,  0.1224,  0.0111],\n",
       "         [ 0.2342,  0.0726, -0.1403,  ...,  0.0104, -0.0112,  0.0374]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 2: tensor([[-0.2176, -0.0995,  0.2730,  ...,  0.0511, -0.4174, -0.4286],\n",
       "         [ 1.3822, -0.1628, -0.5192,  ..., -0.0331, -0.3199, -0.3060],\n",
       "         [-0.1547, -0.1394, -0.4028,  ...,  0.0542, -0.1235, -0.1245],\n",
       "         ...,\n",
       "         [ 0.4801, -0.0596,  0.2832,  ..., -0.7627, -0.1377, -0.1375],\n",
       "         [ 0.0143,  0.0397,  0.6260,  ...,  0.7063, -0.0978,  0.0525],\n",
       "         [ 0.2334,  0.1845,  0.2703,  ..., -0.1080,  0.0861,  0.0658]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 0: tensor([[ 0.4926, -1.2788,  0.2423,  ...,  0.2777, -0.8032, -0.7655],\n",
       "         [-0.1999, -1.4070, -1.5528,  ..., -0.3155, -0.8381, -0.8815],\n",
       "         [-0.0571,  0.0546, -0.0253,  ..., -0.2070, -0.5611, -0.5915],\n",
       "         ...,\n",
       "         [-2.8813,  0.9974, -0.4699,  ...,  0.0292, -0.1655,  0.6536],\n",
       "         [ 1.1672,  0.3690,  0.4592,  ...,  0.2671,  0.0163,  0.3890],\n",
       "         [-0.4435,  0.1095,  0.2843,  ..., -0.4754, -0.1030, -0.3474]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 38: tensor([[-0.6120, -0.1684, -0.7391,  ..., -0.5649, -0.8540, -0.7735],\n",
       "         [ 1.9473, -0.4742, -0.4517,  ...,  0.1383, -0.6889, -0.5932],\n",
       "         [-0.7324,  0.4261,  0.1875,  ..., -0.8989, -0.1099,  0.0194],\n",
       "         ...,\n",
       "         [-0.3569, -0.4148, -0.3361,  ...,  0.3033, -0.3745, -0.4518],\n",
       "         [-0.1124, -0.4894, -0.3167,  ...,  0.4044, -0.2727, -0.4188],\n",
       "         [-0.0995,  0.2730,  0.3124,  ..., -0.2140,  0.3535,  0.5045]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 17: tensor([[-5.8447e-01,  5.6406e-01,  3.7983e-01,  ...,  4.3746e-02,\n",
       "          -2.7742e-01, -2.8075e-01],\n",
       "         [-1.1700e+00, -2.2793e-01,  4.5873e-01,  ...,  1.9938e-01,\n",
       "          -9.6592e-01, -7.4150e-01],\n",
       "         [-8.1312e-01, -1.4382e-01,  5.3736e-01,  ..., -1.2028e+00,\n",
       "           2.7936e-01,  3.5051e-01],\n",
       "         ...,\n",
       "         [-5.1960e-01, -2.4975e-01, -2.3529e-01,  ...,  2.7612e-01,\n",
       "           2.4322e-01,  1.3338e-01],\n",
       "         [ 5.5107e-01,  6.4043e-01,  8.3244e-04,  ...,  5.2590e-01,\n",
       "          -1.7768e-01,  6.2497e-01],\n",
       "         [-6.9325e-01, -5.7987e-01, -5.4068e-01,  ...,  7.2849e-02,\n",
       "          -1.0619e-02,  1.0976e-01]], grad_fn=<CatBackward0>),\n",
       " 3: tensor([[-1.1171, -0.0738,  0.1076,  ..., -0.0219,  0.1822,  0.2709],\n",
       "         [-1.1278,  0.7309,  0.4426,  ...,  0.2389, -0.6075, -0.4076],\n",
       "         [-0.9564,  0.2197, -0.0537,  ...,  0.3305,  0.1957,  0.0920],\n",
       "         ...,\n",
       "         [ 0.5653, -0.6490,  0.0419,  ..., -0.0911, -0.1525,  0.0489],\n",
       "         [ 0.5892, -0.1657, -0.1290,  ..., -0.0902, -0.4276, -0.3290],\n",
       "         [-0.3862,  0.5764,  0.0235,  ..., -0.2368, -0.3305, -0.4374]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 8: tensor([[-0.5235, -0.1955,  0.0662,  ...,  0.0726, -0.3995, -0.4277],\n",
       "         [-0.1576,  0.0124, -0.0996,  ...,  0.0150, -0.7769, -0.7817],\n",
       "         [ 0.7467, -0.1558, -0.0722,  ...,  0.0303, -0.0550, -0.0559],\n",
       "         ...,\n",
       "         [ 0.0762,  0.0742, -0.0664,  ..., -0.0805,  0.1976,  0.1444],\n",
       "         [ 0.4996,  0.0732, -0.0526,  ..., -0.0586, -0.0303, -0.0163],\n",
       "         [ 0.0947,  0.0009, -0.0042,  ..., -0.0638,  0.0391,  0.0715]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 10: tensor([[-0.7379,  0.0652, -0.1833,  ..., -0.1927, -0.0117, -0.0225],\n",
       "         [ 0.3598, -0.3442, -0.2668,  ...,  0.5644, -0.4920, -0.5825],\n",
       "         [-0.2698,  0.4103,  0.1926,  ...,  0.2462, -0.1280, -0.2275],\n",
       "         ...,\n",
       "         [ 0.4147,  0.1391,  0.1310,  ..., -0.1415,  0.4608,  0.4367],\n",
       "         [-0.0820, -0.1410, -0.2692,  ...,  0.2258, -0.1050, -0.1222],\n",
       "         [ 0.0100, -0.0288,  0.2258,  ..., -0.2542,  0.0721,  0.1057]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 23: tensor([[ 0.9579,  0.1589, -0.4436,  ...,  0.7178, -0.5053, -0.5848],\n",
       "         [-0.8263, -0.7021,  1.1107,  ..., -0.5121,  1.4721,  1.3732],\n",
       "         [ 0.2857, -0.2983,  1.3571,  ...,  1.2022,  1.2874,  1.2361],\n",
       "         ...,\n",
       "         [ 1.5147,  0.2105,  0.7213,  ..., -0.2667,  0.6278,  0.2467],\n",
       "         [ 0.5043,  2.8608, -0.7376,  ..., -0.9702, -0.3571, -1.0859],\n",
       "         [-0.8532, -0.1104, -1.1162,  ..., -0.4855,  0.7402,  1.2548]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 50: tensor([[-0.6612, -0.3527, -1.4140,  ...,  0.9830,  0.4303,  0.4474],\n",
       "         [-0.6454,  0.2933,  0.9381,  ...,  1.7992, -1.4692, -1.4800],\n",
       "         [ 0.9380,  1.4186,  1.9726,  ..., -1.7055, -2.1594, -2.1629],\n",
       "         ...,\n",
       "         [-0.0561,  0.2068,  1.5939,  ..., -1.5403,  0.4609,  0.0779],\n",
       "         [-1.0576,  1.0139,  2.0106,  ..., -0.5830,  0.6179, -0.9406],\n",
       "         [ 1.3543, -1.5549, -1.5715,  ...,  1.7683, -1.3895, -0.9016]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 13: tensor([[ 2.1005,  0.3152,  0.2254,  ..., -1.1150, -0.3826, -0.3787],\n",
       "         [-0.6294,  0.0112,  0.0080,  ..., -0.3724, -0.4298, -0.3920],\n",
       "         [ 2.1997, -0.6551, -0.0083,  ...,  1.0593, -0.0832, -0.0823],\n",
       "         ...,\n",
       "         [-0.9033, -0.0128, -0.1073,  ..., -0.2025,  0.5061,  0.6337],\n",
       "         [-2.0609, -0.1403,  0.0719,  ...,  0.2452,  0.3407, -0.0972],\n",
       "         [-1.0236, -0.7583,  0.1394,  ..., -0.2900,  0.4132,  0.3387]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 1: tensor([[-0.3266,  0.5929,  0.4580,  ...,  0.4321,  0.0352,  0.1725],\n",
       "         [-0.5527, -0.2675,  0.5373,  ..., -0.4596, -0.8031, -0.7809],\n",
       "         [ 0.0427, -0.4641,  0.0479,  ..., -0.2723, -0.4498, -0.4387],\n",
       "         ...,\n",
       "         [-0.6519, -0.0288,  0.2964,  ...,  0.1481, -0.3286, -0.5113],\n",
       "         [-0.6853, -0.4574, -0.6547,  ..., -0.0124,  0.7700, -0.1587],\n",
       "         [ 0.5819,  0.3420, -0.4488,  ...,  0.2472,  0.3106,  0.4535]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 18: tensor([[ 0.2033, -0.2101,  0.1440,  ..., -0.4406, -0.2726, -0.2703],\n",
       "         [ 0.0173, -0.1167,  0.2631,  ..., -0.0860, -0.5116, -0.4884],\n",
       "         [ 0.6896, -0.0536, -0.3707,  ..., -0.0392,  0.1800,  0.2081],\n",
       "         ...,\n",
       "         [ 0.5110,  0.5484,  0.3311,  ..., -0.4442, -0.4645, -0.5072],\n",
       "         [-1.2846,  0.1014, -1.1253,  ...,  0.3427, -0.5459, -0.8268],\n",
       "         [ 0.0994, -0.0276,  0.1637,  ..., -0.0711, -0.3441,  0.4181]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 5: tensor([[-0.1967, -0.0224, -0.4009,  ..., -1.0747,  0.9450,  0.8086],\n",
       "         [-0.1482, -0.4541, -0.1230,  ..., -0.4430, -0.5907, -0.5746],\n",
       "         [-0.0145, -0.9241, -0.6946,  ..., -0.5276, -0.1229, -0.0960],\n",
       "         ...,\n",
       "         [ 0.4970,  0.1207, -0.3529,  ..., -0.5519, -0.5117, -0.4777],\n",
       "         [ 0.7593,  0.0882,  0.2602,  ...,  0.5190,  0.6993,  0.3768],\n",
       "         [-1.7069, -0.4921, -0.1332,  ..., -0.0839, -0.0440, -0.0992]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 24: tensor([[-1.3807e-03,  1.0787e-01, -1.8784e+00,  ..., -1.1820e+00,\n",
       "          -6.0456e-01, -6.3993e-01],\n",
       "         [ 6.3565e-01,  1.1176e+00,  1.4518e+00,  ...,  8.3929e-01,\n",
       "           1.9503e+00,  1.9813e+00],\n",
       "         [-5.1820e-01,  4.8884e-01, -3.4165e-01,  ..., -4.6707e-01,\n",
       "          -5.1629e-01, -5.5936e-01],\n",
       "         ...,\n",
       "         [-1.3752e+00,  5.8505e-01, -3.5383e-01,  ..., -6.4925e-01,\n",
       "          -3.9690e-01, -4.5630e-01],\n",
       "         [ 9.1094e-01,  1.4614e-01, -8.3658e-01,  ...,  1.7580e-01,\n",
       "           1.1626e+00,  6.7202e-01],\n",
       "         [ 5.6216e-01, -3.2945e-01,  7.7301e-01,  ...,  6.8091e-01,\n",
       "           9.7198e-01,  8.6859e-01]], grad_fn=<CatBackward0>),\n",
       " 27: tensor([[-1.3171,  1.0885,  0.6575,  ..., -0.8978, -1.2039, -1.1432],\n",
       "         [ 0.4134,  0.4232,  1.0849,  ...,  1.4999,  0.2956,  0.2127],\n",
       "         [ 0.0310, -1.8555,  0.1598,  ..., -3.1568,  1.7977,  1.7821],\n",
       "         ...,\n",
       "         [ 2.3254,  1.9350,  0.3859,  ...,  0.8215,  0.3931, -0.2664],\n",
       "         [ 1.3257,  0.7516, -0.0238,  ..., -0.0599, -0.5516,  0.3310],\n",
       "         [ 2.7539, -1.3343, -0.4969,  ..., -2.1961,  0.7493, -0.3051]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 29: tensor([[ 0.5726, -2.2694,  0.8939,  ...,  3.1433, -1.1398, -1.1465],\n",
       "         [-0.7550, -0.6661, -0.9147,  ...,  0.7800, -1.4758, -1.4398],\n",
       "         [-1.5035, -0.3869, -0.8929,  ...,  0.3081, -0.3158, -0.3112],\n",
       "         ...,\n",
       "         [-1.3437,  0.0896,  0.5235,  ..., -0.3623,  0.9830,  1.0141],\n",
       "         [-0.7950, -3.6134, -1.4172,  ...,  0.5100, -0.7573, -0.9725],\n",
       "         [ 3.6487, -1.6469, -0.8842,  ...,  1.4247, -0.3605,  0.8028]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 6: tensor([[-0.9864,  0.1374, -0.3202,  ...,  0.1713, -0.0746, -0.0565],\n",
       "         [ 0.1592, -0.0420, -0.0477,  ...,  0.1075,  0.1806,  0.2440],\n",
       "         [ 0.2060, -0.0733,  0.0539,  ..., -0.0206,  0.2888,  0.2226],\n",
       "         ...,\n",
       "         [ 0.4584,  0.1024, -0.2013,  ...,  0.1490,  0.1192,  0.0905],\n",
       "         [ 2.4652, -0.1102,  0.2039,  ..., -0.0561, -0.8438, -0.6723],\n",
       "         [ 0.0293, -0.1357,  0.2574,  ..., -0.1091, -0.0658,  0.0378]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 26: tensor([[-0.9728,  0.8028, -0.4418,  ..., -1.0290,  1.0821,  1.0841],\n",
       "         [ 0.0367, -0.1739,  0.0996,  ..., -0.0832, -0.1986, -0.1765],\n",
       "         [-0.5103, -0.7632,  0.1946,  ..., -0.9810, -0.7501, -0.7244],\n",
       "         ...,\n",
       "         [-0.1369, -0.0242,  0.0128,  ...,  0.0602,  0.9183,  0.3845],\n",
       "         [-0.1523, -0.2726, -0.0060,  ..., -0.0208,  0.5257,  0.3971],\n",
       "         [ 0.7423, -1.0078, -0.1123,  ..., -1.0713,  1.1457,  0.5553]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 25: tensor([[-1.3712,  1.5663, -0.2305,  ..., -0.2043, -0.7609, -0.7718],\n",
       "         [-0.4133, -0.2684,  0.1938,  ..., -0.1532,  0.1784,  0.1520],\n",
       "         [ 0.6186,  0.4790, -0.1624,  ...,  0.0315, -0.4446, -0.4371],\n",
       "         ...,\n",
       "         [-1.0244,  0.0802,  0.1126,  ..., -0.2069, -0.8181, -0.0940],\n",
       "         [-1.0810,  1.0697,  0.4896,  ..., -0.4832,  0.8897,  0.0651],\n",
       "         [-1.0224,  0.8712,  0.1622,  ...,  0.0602, -1.9880, -2.1228]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 4: tensor([[ 0.2817,  0.0070,  0.4100,  ..., -0.7289, -1.2051, -1.0491],\n",
       "         [-0.9632,  0.4186,  0.3900,  ..., -0.7257, -0.6439, -0.5513],\n",
       "         [-1.4000,  0.3021, -0.1676,  ..., -0.2885,  0.2818,  0.2216],\n",
       "         ...,\n",
       "         [ 0.1953,  0.5616, -0.1132,  ..., -0.0154, -0.4877, -0.9465],\n",
       "         [-0.3762, -0.3177,  0.4253,  ...,  0.3646, -0.1776, -0.1691],\n",
       "         [ 0.4614,  0.0595, -0.2388,  ..., -0.0289,  0.4891,  0.4487]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 20: tensor([[ 8.5882e-03,  3.8379e-02,  1.9291e-02,  ..., -1.8771e-02,\n",
       "          -1.9748e-02, -4.3939e-02],\n",
       "         [-6.8112e-01,  2.2479e-01,  6.3997e-01,  ...,  5.7117e-02,\n",
       "          -7.8681e-01, -7.6313e-01],\n",
       "         [ 2.2963e+00,  2.4365e-01,  1.7348e-01,  ..., -2.3393e+00,\n",
       "          -2.2450e-01, -2.3030e-01],\n",
       "         ...,\n",
       "         [-3.6049e-01, -7.3941e-02, -4.2411e-01,  ..., -6.2948e-01,\n",
       "           1.9782e-03, -5.1067e-02],\n",
       "         [-2.2634e-01,  2.0030e-01,  4.6418e-02,  ..., -4.7038e-01,\n",
       "           1.1100e-01,  8.4845e-02],\n",
       "         [ 2.1607e+00, -1.3201e-01, -2.5452e-01,  ..., -9.7079e-01,\n",
       "          -3.4940e-01,  4.9795e-01]], grad_fn=<CatBackward0>),\n",
       " 49: tensor([[-0.6330, -0.0715,  0.2613,  ..., -0.0468, -0.3563, -0.3864],\n",
       "         [-0.0325,  0.1773, -0.3897,  ..., -0.1049, -0.3299, -0.3092],\n",
       "         [-0.3943,  0.0056,  0.0520,  ..., -0.0018, -0.2654, -0.2833],\n",
       "         ...,\n",
       "         [-0.0274, -0.0825, -0.0597,  ...,  0.0626,  0.1105,  0.0370],\n",
       "         [ 0.1519, -0.1004,  0.0643,  ...,  0.1124,  0.0587, -0.0831],\n",
       "         [-0.4860,  0.0843, -0.0862,  ..., -0.2153,  0.0819,  0.0953]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 21: tensor([[-0.5648, -0.4875, -0.5395,  ...,  0.3908, -0.4499, -0.4786],\n",
       "         [-0.5502,  0.0192, -0.0531,  ...,  0.0742, -0.3741, -0.3266],\n",
       "         [-0.0744, -0.0865, -0.2203,  ...,  0.3136, -0.4663, -0.3360],\n",
       "         ...,\n",
       "         [ 0.2407, -0.1091, -0.4480,  ..., -0.0424, -0.0342,  0.0799],\n",
       "         [-0.3435, -0.2206,  0.2210,  ...,  0.0980, -0.1803, -0.0561],\n",
       "         [-0.4122, -0.3483, -0.0372,  ...,  0.3443, -0.3543, -0.2157]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 30: tensor([[-1.6889,  0.0711,  0.3215,  ..., -0.6052, -0.8111, -0.7066],\n",
       "         [ 0.3043, -0.2844,  0.0398,  ...,  0.1746, -0.4162, -0.4440],\n",
       "         [ 0.6886, -0.1605, -0.2838,  ...,  0.4262, -0.2673, -0.2874],\n",
       "         ...,\n",
       "         [ 0.4352, -0.2981, -0.2045,  ...,  0.1883,  0.0479,  0.2313],\n",
       "         [-0.1518, -0.0566,  0.0256,  ...,  0.0183,  0.6227,  0.1839],\n",
       "         [ 0.0822, -0.2396, -0.0437,  ...,  0.1146, -0.2838, -0.0207]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 12: tensor([[-0.1045, -0.2854, -0.2818,  ..., -0.6572, -0.4607, -0.4564],\n",
       "         [ 0.0778, -0.2911, -0.2622,  ..., -0.8682, -0.3304, -0.4816],\n",
       "         [ 0.2333, -0.4758, -0.2554,  ...,  0.9021,  0.3079,  0.3373],\n",
       "         ...,\n",
       "         [-0.4753, -0.0342,  0.1480,  ...,  0.5906, -0.6530, -0.7030],\n",
       "         [-0.3847,  0.0941,  0.1251,  ...,  0.6378, -0.6860, -0.8168],\n",
       "         [ 0.3987,  0.0930, -0.0579,  ..., -0.6986,  0.7594,  1.0591]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 42: tensor([[ 1.0470, -0.5882, -0.1749,  ..., -1.0217, -1.0124, -1.0817],\n",
       "         [-0.2214, -1.1539, -0.3471,  ..., -0.2700, -0.1273, -0.2602],\n",
       "         [-0.6969, -0.0237, -1.2543,  ...,  1.8540, -0.2302, -0.1572],\n",
       "         ...,\n",
       "         [-0.6604,  0.4002,  0.0649,  ...,  0.2388, -0.4609, -0.3238],\n",
       "         [-0.7730,  0.3986, -0.3734,  ..., -0.6441,  0.3490,  0.2678],\n",
       "         [ 0.6287,  0.3755,  0.1418,  ..., -1.2827, -1.0311,  0.0295]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 14: tensor([[ 2.1486, -1.2090,  0.7724,  ...,  2.2591, -1.6459, -1.7341],\n",
       "         [-0.8249,  1.1711, -0.9885,  ...,  0.2463,  0.9586,  0.9164],\n",
       "         [ 3.0720,  1.6622,  0.6493,  ..., -0.2719,  0.0521,  0.0097],\n",
       "         ...,\n",
       "         [-1.0033, -0.7855, -0.7233,  ...,  0.3876,  0.8624, -0.1021],\n",
       "         [-0.7875, -1.3140,  2.0176,  ...,  0.4917,  0.3899, -0.7925],\n",
       "         [ 1.5812,  0.8645,  0.6642,  ..., -1.0699, -1.2085,  0.8170]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 33: tensor([[ 1.6923, -0.6194,  0.1507,  ...,  1.0263, -0.5762, -0.6360],\n",
       "         [ 0.8225, -2.5839,  1.7341,  ...,  0.6949, -0.8248, -0.8221],\n",
       "         [-0.1356, -1.5157,  0.2846,  ..., -2.0757,  0.1790,  0.1431],\n",
       "         ...,\n",
       "         [ 0.0391,  1.3634, -0.0354,  ..., -0.2730, -1.3216, -0.2982],\n",
       "         [-1.4452, -1.0750,  1.3289,  ...,  0.0992,  0.3586, -0.1177],\n",
       "         [-2.9497, -0.7442, -0.4614,  ..., -1.8348, -0.1471, -0.7289]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 39: tensor([[-0.0045, -0.0398, -0.0605,  ...,  0.0023, -0.1894, -0.1987],\n",
       "         [ 0.0401,  0.0230,  0.0184,  ..., -0.0955, -0.2575, -0.2451],\n",
       "         [ 0.0578, -0.0560, -0.1209,  ...,  0.0577, -0.1921, -0.2309],\n",
       "         ...,\n",
       "         [ 0.5802,  0.1006, -0.0613,  ..., -0.0351, -0.2004, -0.0275],\n",
       "         [ 1.4203,  0.0056, -0.1083,  ...,  0.1079,  0.0947,  0.0180],\n",
       "         [ 0.4310, -0.0132,  0.1068,  ..., -0.1598, -0.2104, -0.0644]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 40: tensor([[ 1.0443, -1.0614, -0.6952,  ..., -0.3718,  2.0209,  2.0023],\n",
       "         [-0.3727,  0.5412,  0.6200,  ..., -0.0535, -0.6896, -0.6869],\n",
       "         [ 4.3478, -0.0060,  0.2881,  ...,  0.1880,  1.1818,  1.2103],\n",
       "         ...,\n",
       "         [-0.2781, -0.3452,  0.0127,  ...,  0.0521,  0.5104,  0.5464],\n",
       "         [-1.7294,  0.0723,  0.0212,  ...,  0.0153,  0.0376,  0.5011],\n",
       "         [-0.1854,  0.2647, -0.0406,  ..., -0.3752, -0.3446, -1.1829]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 51: tensor([[-0.2088,  0.4394, -0.0986,  ...,  0.0027, -0.5804, -0.6034],\n",
       "         [-0.7395, -0.1092,  0.4036,  ...,  0.0480, -0.2010, -0.2089],\n",
       "         [-0.4724, -0.3871,  0.0646,  ..., -0.8201,  0.2936,  0.2916],\n",
       "         ...,\n",
       "         [-0.4517, -0.1163,  0.0839,  ...,  0.1320,  0.0581,  0.0157],\n",
       "         [-0.9454,  0.0450,  0.0460,  ...,  1.1941,  0.5869,  0.3092],\n",
       "         [-0.1936, -0.0444, -0.0821,  ..., -0.3466, -1.6416, -0.1408]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 31: tensor([[-0.8005,  0.6305, -0.4014,  ..., -0.5725, -0.5797, -0.6229],\n",
       "         [-0.2564,  0.4514, -0.2589,  ..., -0.4486, -0.6611, -0.6346],\n",
       "         [-0.1006,  0.5006, -0.4752,  ..., -0.5360, -0.4601, -0.4390],\n",
       "         ...,\n",
       "         [-0.5721,  0.4257, -0.2230,  ..., -0.2265, -0.3823, -0.3585],\n",
       "         [ 0.5117, -0.2659,  0.2248,  ...,  0.2583,  0.4266,  0.1888],\n",
       "         [-0.2721, -0.4460,  0.4846,  ...,  0.1710,  0.1114,  0.3838]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 43: tensor([[ 0.0544, -1.7664, -0.9991,  ...,  0.4607,  0.8124,  0.8138],\n",
       "         [ 0.3176, -0.3023, -1.8969,  ..., -0.4833,  0.3159,  0.3071],\n",
       "         [ 0.8604,  1.1654,  0.1071,  ...,  0.4113, -2.5780, -2.6090],\n",
       "         ...,\n",
       "         [-0.4641,  0.1051,  0.5139,  ..., -0.7124, -0.2279, -1.9249],\n",
       "         [-1.0259,  0.0061, -0.8705,  ...,  0.9343,  0.4710, -0.0550],\n",
       "         [-1.0107, -0.6425,  0.0045,  ..., -1.0645,  0.1889, -0.1612]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 46: tensor([[ 0.2688, -0.4523,  0.2873,  ...,  0.0810,  0.8307,  0.8151],\n",
       "         [-0.3675,  0.0413,  0.0342,  ..., -0.4062, -0.4942, -0.5716],\n",
       "         [-0.5105,  0.3326, -0.4944,  ...,  0.2748,  0.1587,  0.2240],\n",
       "         ...,\n",
       "         [ 1.3286, -0.0620, -0.2612,  ...,  0.0890,  0.3478,  0.4932],\n",
       "         [ 1.6087,  0.9770, -0.2204,  ...,  0.2442,  0.6451,  0.4459],\n",
       "         [-0.5671, -0.1926, -0.3415,  ..., -0.1909,  0.1715,  0.5757]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 47: tensor([[ 1.0100, -2.1173,  0.3664,  ...,  1.3907, -1.5907, -1.5878],\n",
       "         [-1.2263, -0.2493,  0.0356,  ...,  0.1251, -0.7898, -0.8146],\n",
       "         [ 2.4363,  2.0974,  0.2661,  ...,  2.2245, -0.0636, -0.0851],\n",
       "         ...,\n",
       "         [-0.3114, -2.1317,  1.3484,  ..., -0.3224, -1.1957, -0.7704],\n",
       "         [ 1.0556, -0.3675,  0.2520,  ...,  0.0531, -1.1815,  0.1555],\n",
       "         [ 0.9561, -0.1809, -0.1231,  ..., -0.1509, -0.6448, -0.4267]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 35: tensor([[-0.2245,  0.5684,  0.0984,  ..., -0.2200, -0.4255, -0.5369],\n",
       "         [-0.3113,  0.5521, -0.4341,  ..., -0.3482, -0.8602, -0.8726],\n",
       "         [ 0.3567,  0.1343,  0.6139,  ...,  0.3794, -0.0384, -0.2632],\n",
       "         ...,\n",
       "         [-0.3490, -0.1445, -0.0763,  ...,  0.0892, -0.2572, -0.4241],\n",
       "         [-0.2406, -0.3622, -0.2762,  ..., -0.1473, -0.0906, -0.0385],\n",
       "         [-0.3103, -0.3096, -0.1696,  ..., -0.0388, -0.0342, -0.0665]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 32: tensor([[-0.1813,  0.2081,  0.0288,  ..., -0.0965, -0.4243, -0.4064],\n",
       "         [-0.9563, -0.1656, -0.1413,  ...,  0.0149, -0.4051, -0.3247],\n",
       "         [-0.6298, -0.0330, -0.7379,  ...,  0.6170, -0.4850, -0.4915],\n",
       "         ...,\n",
       "         [-1.5888,  0.0463,  0.0326,  ...,  0.0139, -0.0406, -0.0382],\n",
       "         [-0.5146, -0.0363, -0.0848,  ...,  0.0309,  0.0410, -0.0183],\n",
       "         [-0.4238,  0.2054,  0.0398,  ..., -0.2815, -0.0573, -0.1786]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 9: tensor([[ 0.2352,  0.9360,  0.1259,  ..., -0.9062,  1.0286,  1.0398],\n",
       "         [-0.9016, -0.2043, -0.1681,  ...,  0.5210, -0.6028, -0.6094],\n",
       "         [-0.9616, -0.4085,  0.3085,  ...,  0.2968, -0.4894, -0.4801],\n",
       "         ...,\n",
       "         [ 0.2265, -0.1297,  0.0656,  ..., -1.3526,  1.2573,  0.4433],\n",
       "         [ 0.1513,  0.2663, -0.1872,  ..., -0.4200, -0.5732,  0.0920],\n",
       "         [-0.6842,  0.1319, -0.7208,  ...,  1.2221, -0.6431, -0.5192]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 41: tensor([[ 1.6377, -0.9827, -1.3966,  ..., -0.3118,  0.5951,  0.5750],\n",
       "         [-0.0767,  1.6111,  0.3210,  ...,  0.0264, -0.3070, -0.3280],\n",
       "         [ 0.0082, -0.5391,  0.3314,  ...,  0.2062,  0.3433,  0.3394],\n",
       "         ...,\n",
       "         [-0.0780, -0.7597,  0.1040,  ...,  0.4740, -0.1004, -0.3334],\n",
       "         [-0.1733,  0.6264, -0.1407,  ..., -0.1191,  0.1554,  0.1061],\n",
       "         [ 0.3981, -0.2848,  0.0760,  ..., -0.0434,  0.3754,  0.0567]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 44: tensor([[ 0.0964,  0.0120,  0.2695,  ...,  0.3203,  0.1131,  0.0698],\n",
       "         [-0.1148, -0.4142, -0.0587,  ..., -0.1599, -0.1172, -0.2185],\n",
       "         [-0.4236, -0.2497, -0.1397,  ..., -0.2896, -0.3150, -0.2669],\n",
       "         ...,\n",
       "         [ 0.0198, -0.1401, -0.1922,  ..., -0.0946, -0.0860, -0.1821],\n",
       "         [-0.1933, -0.0093, -0.1779,  ..., -0.1862, -0.0218,  0.1166],\n",
       "         [-0.1110, -0.1855, -0.1360,  ...,  0.0109, -0.0973, -0.1174]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 11: tensor([[ 0.0043, -0.0379,  0.1833,  ...,  0.1207, -0.1508, -0.0750],\n",
       "         [ 0.0424,  0.2524, -0.1870,  ..., -0.2434,  0.1768,  0.0463],\n",
       "         [ 0.1293, -0.1843,  0.1428,  ...,  0.1820, -0.0307, -0.0237],\n",
       "         ...,\n",
       "         [ 0.1685,  0.2564, -0.1037,  ..., -0.2733,  0.2702,  0.2268],\n",
       "         [ 0.0730, -0.0159, -0.3402,  ..., -0.1650,  0.1360,  0.2104],\n",
       "         [ 0.1103,  0.1203, -0.2877,  ..., -0.2270,  0.1044,  0.0913]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 48: tensor([[ 0.1769, -0.9905,  0.0318,  ..., -0.1931, -0.0728,  0.0045],\n",
       "         [ 0.1312, -0.1884,  0.1559,  ...,  0.1425, -0.3514, -0.2063],\n",
       "         [ 0.2469, -0.1718,  0.1782,  ..., -0.0174, -0.2556, -0.1916],\n",
       "         ...,\n",
       "         [-0.1887, -0.1415,  0.1855,  ...,  0.4640, -0.2874, -0.4636],\n",
       "         [-0.1790, -0.2763,  0.2438,  ...,  0.5118, -0.2657, -0.1829],\n",
       "         [-0.1633, -0.4064,  0.2977,  ..., -0.2046, -0.2742,  0.0161]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 36: tensor([[ 0.5026,  0.4059, -1.5814,  ..., -0.2608, -0.8377, -0.8591],\n",
       "         [ 0.0817,  1.1565,  1.0409,  ..., -0.0922, -0.6713, -0.7045],\n",
       "         [-0.7666,  0.0724,  0.2738,  ..., -0.1758,  0.7615,  0.7612],\n",
       "         ...,\n",
       "         [ 0.1931,  0.1441,  0.0972,  ...,  0.0456,  0.0813,  0.0721],\n",
       "         [ 1.1605,  0.2013,  0.7137,  ...,  0.0177,  0.9001,  0.6421],\n",
       "         [-0.5596, -0.4955,  0.0950,  ..., -0.0937,  0.6744, -0.2914]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 37: tensor([[-0.4196, -0.1602, -0.1431,  ...,  0.1302, -0.0026, -0.0099],\n",
       "         [ 0.1665,  0.2571,  0.1081,  ..., -0.3203, -0.3103, -0.4081],\n",
       "         [-0.3131, -0.1527, -0.1306,  ...,  0.1138,  0.0060, -0.0278],\n",
       "         ...,\n",
       "         [ 0.4848, -0.1292,  0.1663,  ...,  0.1204,  0.4281,  0.7507],\n",
       "         [ 0.2001,  0.1016,  0.1922,  ..., -0.2049, -0.1501, -0.1484],\n",
       "         [ 0.7597, -0.1200, -0.1757,  ...,  0.0347, -0.1493, -0.4196]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 16: tensor([[ 0.1102, -0.2316,  0.0867,  ..., -0.1797, -0.1197,  0.0110],\n",
       "         [-0.3758,  0.1615, -0.3251,  ...,  0.1395, -0.4258, -0.2906],\n",
       "         [-0.0496, -0.0728,  0.0070,  ..., -0.4018, -0.1653, -0.1214],\n",
       "         ...,\n",
       "         [ 0.1571,  0.1400,  0.2108,  ..., -0.0065,  0.1658,  0.2373],\n",
       "         [ 0.1264, -0.0062,  0.2860,  ..., -0.0625,  0.2147,  0.1576],\n",
       "         [-0.0396,  0.0834, -0.1736,  ..., -0.0815, -0.0650, -0.0556]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 28: tensor([[ 0.7807, -0.0388,  0.1268,  ..., -0.6395, -0.5948, -0.6084],\n",
       "         [ 1.3965, -0.2748, -0.2068,  ...,  0.0142, -0.8141, -0.7006],\n",
       "         [-0.7472, -0.5572,  0.4397,  ..., -0.1392, -0.5092, -0.5206],\n",
       "         ...,\n",
       "         [-0.1287,  0.3961, -0.3619,  ..., -0.3968,  0.1445,  0.1711],\n",
       "         [-0.2217,  0.4115, -0.1846,  ..., -0.3336, -0.0327, -0.0419],\n",
       "         [ 0.1148, -0.0092,  0.4547,  ...,  0.2779,  0.1414,  0.3151]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 7: tensor([[-0.4387, -0.5963,  0.7338,  ...,  0.6550, -0.5881, -0.5591],\n",
       "         [-0.3814,  0.1563, -0.4468,  ...,  0.0875, -0.6374, -0.4720],\n",
       "         [-1.1033, -0.0102, -0.2233,  ..., -0.1662, -0.5637, -0.4231],\n",
       "         ...,\n",
       "         [-0.0466,  0.1967, -0.2047,  ...,  0.4292, -0.0714, -0.3379],\n",
       "         [ 0.1937, -0.0043,  0.0402,  ..., -0.4535, -0.0997, -0.1052],\n",
       "         [ 0.1022,  0.4428, -0.0435,  ...,  0.0297,  0.0567, -0.1192]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 34: tensor([[-0.3397,  1.0929,  0.7913,  ...,  0.6650,  0.3981,  0.4447],\n",
       "         [-0.4729,  0.1446, -0.6247,  ...,  0.1914, -0.7006, -0.6240],\n",
       "         [ 0.8658,  0.9208, -2.5357,  ...,  1.8131,  0.1136,  0.0570],\n",
       "         ...,\n",
       "         [ 1.9657,  0.9846,  0.3571,  ...,  0.5223,  0.0079,  0.2303],\n",
       "         [-1.0884,  1.0014,  0.5687,  ...,  0.3508,  1.2041,  0.9775],\n",
       "         [-0.2833, -0.3314, -0.8630,  ..., -0.3918,  0.7312,  1.4720]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " 22: tensor([[ 0.0181, -0.1976, -0.1683,  ..., -0.0544,  0.1741,  0.2085],\n",
       "         [ 2.1838,  0.5414,  0.5497,  ..., -0.2198, -0.3331, -0.3768],\n",
       "         [ 0.0138,  0.0429, -0.1217,  ...,  0.0668, -0.0062,  0.0410],\n",
       "         ...,\n",
       "         [-0.3132, -0.2331,  0.5481,  ..., -0.0223,  0.0675, -0.1248],\n",
       "         [ 0.2861, -0.0889, -0.0695,  ...,  0.0351,  0.0411, -0.1431],\n",
       "         [ 0.5008, -0.0929, -0.3894,  ..., -0.2938, -0.0775,  0.0541]],\n",
       "        grad_fn=<CatBackward0>)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
