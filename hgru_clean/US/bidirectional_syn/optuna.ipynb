{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/mvilenko/Library/CloudStorage/OneDrive-PayPal/hgru_clean/US/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the required packages\n",
    "import optuna\n",
    "from optuna.samplers import GridSampler, RandomSampler, TPESampler\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "#from transformers import AdamW\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "import itertools\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from model.GRU_model import *\n",
    "from pipeline_config import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 1\n",
      "HiddenSize: 64\n",
      "LayersDim: 1\n",
      "OutputDim: 1\n",
      "DropoutProb: 0.0\n",
      "Lr : 0.1\n",
      "Epochs : 30\n"
     ]
    }
   ],
   "source": [
    "print(f'Features: {Features}')\n",
    "print(f'HiddenSize: {HiddenSize}')\n",
    "print(f'LayersDim: {LayersDim}')\n",
    "print(f'OutputDim: {OutputDim}')\n",
    "print(f'DropoutProb: {DropoutProb}')\n",
    "print(f'Lr : {Lr}')\n",
    "print(f'Epochs : {Epochs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmdir(dirc):\n",
    "    dirc = Path(dirc)\n",
    "    for itm in dirc.iterdir():\n",
    "        if itm.is_dir():\n",
    "            rmdir(itm)\n",
    "            print(\"Deleting\", itm, \".... successful.\")\n",
    "        else:\n",
    "            itm.unlink()\n",
    "    dirc.rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    with open(train_dataset_path, 'rb') as f:\n",
    "        train_dataset_dict = pickle.load(f)\n",
    "        \n",
    "    with open(test_dataset_path, 'rb') as f:\n",
    "        test_dataset_dict = pickle.load(f)\n",
    "\n",
    "    with open(category_id_to_category_name_path, 'rb') as f:\n",
    "        category_id_to_name_dict = pickle.load(f)\n",
    "        \n",
    "    with open(categories_per_indent_path, 'rb') as f:\n",
    "        categories_per_indent_dict = pickle.load(f)\n",
    "\n",
    "    with open(son_parent_path, 'rb') as f:\n",
    "        son_parent_dict = pickle.load(f)\n",
    "\n",
    "    with open(parent_to_son_list_path, 'rb') as f:\n",
    "        parent_to_son_list_dict = pickle.load(f)\n",
    "\n",
    "    with open(hgru_model_weights_path, 'rb') as f:\n",
    "        hgru_weight_dict = pickle.load(f)\n",
    "\n",
    "    with open(coefficient_dict_path, 'rb') as f:\n",
    "        coefficient_dict = pickle.load(f)\n",
    "    \n",
    "    weights_path = weightspath\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------#\n",
    "    # deleting files in models_weights folder\n",
    "    path = os.getcwd()\n",
    "    desired_path = path + '/models_weights/'\n",
    "\n",
    "    is_empty = not any(Path(desired_path).iterdir())\n",
    "    if is_empty==False:\n",
    "        rmdir(Path(desired_path))\n",
    "    \n",
    "    # deleting empty folder\n",
    "    try:\n",
    "        os.rmdir(desired_path)\n",
    "    except OSError:\n",
    "        print (\"Deletion of the directory %s failed\" % desired_path)\n",
    "    else:\n",
    "        print (\"Successfully deleted the directory %s\" % desired_path)\n",
    "\n",
    "    # creating new folder\n",
    "    try:\n",
    "        os.mkdir(desired_path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % desired_path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s\" % desired_path)\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "    loss_coef_1 = trial.suggest_float('loss_coef_1',  1e-10, 1e-4, log=True)\n",
    "    loss_coef_2 = trial.suggest_float('loss_coef_2',  1e-10, 1e-4, log=True)\n",
    "    loss_coef_3 = trial.suggest_float('loss_coef_3',  1e-10, 1e-4, log=True)\n",
    "    alpha = trial.suggest_float('alpha',  1e-10, 1, log=True)\n",
    "    Lr = trial.suggest_float('Lr', 1e-5, 1e-1, log=True)\n",
    "\n",
    "    bi_directional_models = {}\n",
    "    num_categories = 0\n",
    "\n",
    "    for indent in sorted(list(categories_per_indent_dict.keys()),reverse=True):\n",
    "        for category in categories_per_indent_dict[indent]:\n",
    "            num_categories +=1\n",
    "            print(f'num categories: {num_categories}')\n",
    "            category_name = category_id_to_name_dict[category]\n",
    "            print(f'category id|name: {category}|{category_name}')\n",
    "\n",
    "            if int(indent) == 0 or son_parent_dict[category] not in categories_per_indent_dict[indent-1]: #or (category, son_parent_dict[category]) not in son_parent_corr_dict.keys():\n",
    "                loss_coef_1 = 0\n",
    "\n",
    "            if (category not in list(parent_to_son_list_dict.keys())) or (set(parent_to_son_list_dict[category]['sons']).issubset(set(categories_per_indent_dict[indent+1]))):\n",
    "                loss_coef_2 = 0\n",
    " \n",
    "            print('------------------------------------------------------------------')\n",
    "\n",
    "            train_dataloader, test_dataloader = create_dataloader(train_dataset_dict[category_name], test_dataset_dict[category_name])\n",
    "            model = GRUModel(input_dim=Features, hidden_dim=HiddenSize, layer_dim=LayersDim, output_dim=OutputDim, dropout_prob=DropoutProb, seed=0)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=Lr)\n",
    "            model.to(Device)\n",
    "            saving_param_path = weights_path+category_name+'.pt'\n",
    "            \n",
    "            min_error = training_and_evaluation(trial, model, indent, train_dataloader, test_dataloader, optimizer, category, hgru_weight_dict, coefficient_dict, son_parent_dict, parent_to_son_list_dict, category_id_to_name_dict, loss_coef_1, loss_coef_2, loss_coef_3, alpha, saving_param_path)\n",
    "            \n",
    "            bi_directional_models[category] = min_error\n",
    "\n",
    "    average_error = sum(list(bi_directional_models.values()))/len(list(bi_directional_models.values()))\n",
    "    \n",
    "    return average_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'optuna.samplers' has no attribute 'GPSampler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/88/mkn0vj0s1kl1179x22s6j2h80000gq/T/ipykernel_60845/1256152815.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGPSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'optuna.samplers' has no attribute 'GPSampler'"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "    \n",
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.GPSampler())\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best trial\n",
    "best_trial = study.best_trial\n",
    "\n",
    "# Print the best trial\n",
    "print('Best trial:')\n",
    "print(f'Value: {best_trial.value}')\n",
    "print('Parameters:')\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "    \n",
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.RandomSampler())\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best trial\n",
    "best_trial = study.best_trial\n",
    "\n",
    "# Print the best trial\n",
    "print('Best trial:')\n",
    "print(f'Value: {best_trial.value}')\n",
    "print('Parameters:')\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "    \n",
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best trial\n",
    "best_trial = study.best_trial\n",
    "\n",
    "# Print the best trial\n",
    "print('Best trial:')\n",
    "print(f'Value: {best_trial.value}')\n",
    "print('Parameters:')\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
